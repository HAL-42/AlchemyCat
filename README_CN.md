# Alchemy Cat

<div align="center">

![GitHub commit activity](https://img.shields.io/github/commit-activity/y/HAL-42/AlchemyCat)
<img src="https://img.shields.io/github/stars/HAL-42/AlchemyCat?color=yellow" alt="Stars">
<img src="https://img.shields.io/github/issues/HAL-42/AlchemyCat?color=red" alt="Issues">
![GitHub License](https://img.shields.io/github/license/HAL-42/AlchemyCat?color=cyan)
<br>
[![PyPI version](https://badge.fury.io/py/alchemy-cat.svg)](https://badge.fury.io/py/alchemy-cat)
![PyPI - Downloads](https://img.shields.io/pypi/dm/alchemy-cat?color=yellow)
<img src="https://img.shields.io/badge/python-3.9-purple.svg" alt="Python"> <br>

</div>

<div align="center">
<a href="README.md">English</a> | <a href="README_CN.md">ä¸­æ–‡</a>
</div>

![banner](https://raw.githubusercontent.com/HAL-42/AlchemyCat/master/docs/figs/dl_config_logo.png)

<div align="center">

[ä»‹ç»](#ä»‹ç») | [å®‰è£…](#å®‰è£…) | [è¿ç§»](#è¿ç§») | [æ–‡æ¡£](#æ–‡æ¡£)

</div>

# <div align="center">ğŸš€ ä»‹ç»</div>

<div align="center">
  AlchemyCat ä¸ºæ·±åº¦å­¦ä¹ æä¾›äº†ä¸€å¥—å…ˆè¿›çš„é…ç½®ç³»ç»Ÿã€‚<br> è¯­æ³•<strong>ç®€å•ä¼˜é›…</strong>ï¼Œæ”¯æŒç»§æ‰¿ã€ç»„åˆã€ä¾èµ–ä»¥<strong>æœ€å°åŒ–é…ç½®å†—ä½™</strong>ï¼Œå¹¶æ”¯æŒ<strong>è‡ªåŠ¨è°ƒå‚</strong>ã€‚
</div>

ä¸‹è¡¨å¯¹æ¯”äº† AlchemyCat å’Œå…¶ä»–é…ç½®ç³»ç»Ÿï¼ˆğŸ˜¡ä¸æ”¯æŒï¼ŒğŸ¤”æœ‰é™æ”¯æŒï¼ŒğŸ¥³æ”¯æŒï¼‰ï¼š

| åŠŸèƒ½    | argparse | yaml | YACS | mmcv | AlchemyCat |
|-------|----------|------|------|------|------------|
| å¯å¤ç°   | ğŸ˜¡       | ğŸ¥³   | ğŸ¥³   | ğŸ¥³   | ğŸ¥³         |
| IDEè·³è½¬ | ğŸ˜¡       | ğŸ˜¡   | ğŸ¥³   | ğŸ¥³   | ğŸ¥³         |
| ç»§æ‰¿    | ğŸ˜¡       | ğŸ˜¡   | ğŸ¤”   | ğŸ¤”   | ğŸ¥³         |
| ç»„åˆ    | ğŸ˜¡       | ğŸ˜¡   | ğŸ¤”   | ğŸ¤”   | ğŸ¥³         |
| ä¾èµ–    | ğŸ˜¡       | ğŸ˜¡   | ğŸ˜¡   | ğŸ˜¡   | ğŸ¥³         |
| è‡ªåŠ¨è°ƒå‚  | ğŸ˜¡       | ğŸ˜¡   | ğŸ˜¡   | ğŸ˜¡   | ğŸ¥³         |

AlchemyCat å›Šæ‹¬äº†æ­¤å‰ "SOTA" é…ç½®ç³»ç»Ÿæä¾›çš„æ‰€æœ‰åŠŸèƒ½ï¼Œä¸”å……åˆ†è€ƒè™‘äº†å„ç§ç‰¹æ®Šæƒ…å†µï¼Œç¨³å®šæ€§æœ‰ä¿éšœã€‚

AlchemyCat çš„ç‹¬åˆ°ä¹‹å¤„åœ¨äºï¼š
* æ”¯æŒç»§æ‰¿ã€ç»„åˆæ¥å¤ç”¨å·²æœ‰é…ç½®ï¼Œæœ€å°åŒ–é…ç½®å†—ä½™ã€‚
* æ”¯æŒé…ç½®é¡¹é—´ç›¸äº’ä¾èµ–ï¼Œä¸€å¤„ä¿®æ”¹ï¼Œå¤„å¤„ç”Ÿæ•ˆï¼Œå¤§å¤§é™ä½ä¿®æ”¹é…ç½®æ—¶çš„å¿ƒæ™ºè´Ÿæ‹…ã€‚
* æä¾›ä¸€å°è‡ªåŠ¨è°ƒå‚æœºï¼Œåªéœ€å¯¹é…ç½®æ–‡ä»¶åšä¸€ç‚¹ç‚¹ä¿®æ”¹ï¼Œå³å¯å®ç°è‡ªåŠ¨è°ƒå‚å¹¶æ€»ç»“ã€‚
* é‡‡ç”¨äº†æ›´åŠ ç®€å•ä¼˜é›…ã€pythonic çš„è¯­æ³•ï¼Œé™„å¸¦å¤§é‡å¼€ç®±å³ç”¨çš„å®ç”¨æ–¹æ³•ã€å±æ€§ã€‚

å¦‚æœæ‚¨å·²ç»åœ¨ä½¿ç”¨ä¸Šè¡¨ä¸­æŸä¸ªé…ç½®ç³»ç»Ÿï¼Œè¿ç§»åˆ° AlchemyCat å‡ ä¹æ˜¯é›¶æˆæœ¬çš„ã€‚èŠ±15åˆ†é’Ÿé˜…è¯»ä¸‹é¢çš„æ–‡æ¡£ï¼Œå¹¶å°† AlchemyCat è¿ç”¨åˆ°é¡¹ç›®ä¸­ï¼Œä»æ­¤ä½ çš„GPUå°†æ°¸æ— ç©ºé—²ï¼

# <div align="center">ğŸ“¦ å®‰è£…</div>
```bash
pip install alchemy-cat
```

# <div align="center">ğŸšš è¿ç§»</div>
<details>
<summary> å¦‚ä½•ä» YAML / YACS / MMCV è¿ç§» </summary>

Ïƒ`âˆ€Â´)Ïƒ å¼€ç©ç¬‘çš„å•¦ï¼ä¸éœ€è¦è¿ç§»ã€‚AlchemyCat æ”¯æŒç›´æ¥è¯»å†™ YAMLã€YACSã€MMCV é…ç½®æ–‡ä»¶ï¼š
```python
from alchemy_cat.dl_config import load_config, Config

# READ YAML / YACS / MMCV config to alchemy_cat.Config
cfg = load_config('path/to/yaml_config.yaml or yacs_config.py or mmcv_config.py')
# Init alchemy_cat.Config with YAML / YACS / MMCV config
cfg = Config('path/to/yaml_config.yaml or yacs_config.py or mmcv_config.py')
# alchemy_cat.Config inherits from YAML / YACS / MMCV config
cfg = Config(caps='path/to/yaml_config.yaml or yacs_config.py or mmcv_config.py')

print(cfg.model.backbone)  # Access config item

cfg.save_yaml('path/to/save.yaml')  # Save to YAML config
cfg.save_mmcv('path/to/save.py')    # Save to MMCV config
cfg.save_py('path/to/save.py')      # Save to AlchemyCat config
```
æˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªè„šæœ¬ï¼Œç”¨äºè½¬æ¢ä¸åŒé…ç½®æ ¼å¼ï¼š
```bash
python -m alchemy_cat.dl_config.from_x_to_y --x X --y Y --y_type=yaml/mmcv/alchemy-cat
```
å…¶ä¸­ï¼š
* `--x`ï¼šæºé…ç½®æ–‡ä»¶è·¯å¾„ï¼Œå¯ä»¥æ˜¯ YAML / YACS / MMCV / AlchemyCat é…ç½®æ–‡ä»¶ã€‚
* `--y`ï¼šç›®æ ‡é…ç½®æ–‡ä»¶è·¯å¾„ã€‚
* `--y_type`ï¼šç›®æ ‡é…ç½®çš„æ ¼å¼ï¼Œå¯ä»¥æ˜¯ `yaml`ã€`mmcv` æˆ– `alchemy-cat`ã€‚

</details>

# <div align="center">ğŸ“– æ–‡æ¡£ </div>

## åŸºç¡€ä½¿ç”¨
AlchemyCat ç¡®ä¿é…ç½®ä¸å®éªŒä¸€ä¸€å¯¹åº”ï¼Œå‘ˆåŒå°„å…³ç³»ï¼š
```text
config C + algorithm code A â€”â€”> reproducible experiment E(C, A)
```
å®éªŒç›®å½•æ˜¯è‡ªåŠ¨åˆ›å»ºçš„ï¼Œä¸”ä¸é…ç½®æ–‡ä»¶æœ‰ç›¸åŒçš„ç›¸å¯¹è·¯å¾„ã€‚è·¯å¾„å¯ä»¥æ˜¯å¤šçº§ç›®å½•ï¼Œè·¯å¾„ä¸­å¯ä»¥æœ‰ç©ºæ ¼ã€é€—å·ã€ç­‰å·ç­‰ã€‚è¿™ä¾¿äºåˆ†é—¨åˆ«ç±»åœ°ç®¡ç†å®éªŒã€‚è­¬å¦‚ï¼š
```text
.
â”œâ”€â”€ configs
â”‚Â Â  â”œâ”€â”€ MNIST
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ resnet18,wd=1e-5@run2
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ cfg.py
â”‚Â Â  â”‚Â Â  â””â”€â”€ vgg,lr=1e-2
â”‚Â Â  â”‚Â Â      â””â”€â”€ cfg.py
â”‚Â Â  â””â”€â”€ VOC2012
â”‚Â Â      â””â”€â”€ swin-T,Î³=10
â”‚Â Â          â””â”€â”€ 10 epoch
â”‚Â Â              â””â”€â”€ cfg.py
â””â”€â”€ experiment
    â”œâ”€â”€ MNIST
    â”‚Â Â  â”œâ”€â”€ resnet18,wd=1e-5@run2
    â”‚Â Â  â”‚Â Â  â””â”€â”€ xxx.log
    â”‚Â Â  â””â”€â”€ vgg,lr=1e-2
    â”‚Â Â      â””â”€â”€ xxx.log
    â””â”€â”€ VOC2012
        â””â”€â”€ swin-T,Î³=10
            â””â”€â”€ 10 epoch
                â””â”€â”€ xxx.log
```
**æœ€ä½³å®è·µï¼šåœ¨`cfg.py`æ—è¾¹åˆ›å»ºä¸€ä¸ª`__init__.py`ï¼ˆä¸€èˆ¬IDEä¼šè‡ªåŠ¨åˆ›å»ºï¼‰ï¼Œå¹¶é¿å…è·¯å¾„ä¸­å«æœ‰'.'ã€‚éµå®ˆè¯¥æœ€ä½³å®è·µæœ‰åŠ©äº IDE è°ƒè¯•ï¼Œä¸”èƒ½å¤Ÿåœ¨`cfg.py`ä¸­ä½¿ç”¨ç›¸å¯¹å¯¼å…¥ã€‚**


è®©æˆ‘ä»¬ä»ä¸€ä¸ªä¸å®Œæ•´çš„ä¾‹å­å¼€å§‹ï¼Œäº†è§£å¦‚ä½•ä¹¦å†™å’ŒåŠ è½½é…ç½®ã€‚æˆ‘ä»¬é¦–å…ˆåˆ›å»º[é…ç½®æ–‡ä»¶](alchemy_cat/dl_config/examples/configs/mnist/plain_usage/cfg.py)ï¼š
```python
# -- [INCOMPLETE] configs/mnist/plain_usage/cfg.py --

from torchvision.datasets import MNIST
from alchemy_cat.dl_config import Config

cfg = Config()

cfg.rand_seed = 0

cfg.dt.cls = MNIST
cfg.dt.ini.root = '/tmp/data'
cfg.dt.ini.train = True

# ... Code Omitted.
```
è¿™é‡Œæˆ‘ä»¬é¦–å…ˆå®ä¾‹åŒ–ä¸€ä¸ª`Config`ç±»åˆ«å¯¹è±¡`cfg`ï¼Œéšåé€šè¿‡å±æ€§æ“ä½œ`.`æ¥æ·»åŠ é…ç½®é¡¹ã€‚é…ç½®é¡¹å¯ä»¥æ˜¯ä»»æ„ python å¯¹è±¡ï¼ŒåŒ…æ‹¬å‡½æ•°ã€æ–¹æ³•ã€ç±»ã€‚

**æœ€ä½³å®è·µï¼šæˆ‘ä»¬æ¨èç›´æ¥åœ¨é…ç½®é¡¹ä¸­æŒ‡å®šå‡½æ•°æˆ–ç±»ï¼Œè€Œä¸æ˜¯é€šè¿‡å­—ç¬¦ä¸²/ä¿¡å·é‡æ¥æ§åˆ¶ç¨‹åºè¡Œä¸ºã€‚å‰è€…æ”¯æŒ IDE è·³è½¬ï¼Œä¾¿äºé˜…è¯»å’Œè°ƒè¯•ã€‚**

`Config`æ˜¯python `dict`çš„å­ç±»ï¼Œä¸Šé¢ä»£ç å®šä¹‰äº†ä¸€ä¸ª**æ ‘ç»“æ„**çš„åµŒå¥—å­—å…¸ï¼š
```text
>>> print(cfg.to_dict())
{'rand_seed': 0,
 'dt': {'cls': <class 'torchvision.datasets.mnist.MNIST'>,
        'ini': {'root': '/tmp/data', 'train': True}}}
```
`Config` å®ç°äº† Python `dict`çš„æ‰€æœ‰APIï¼š 
```test
>>> cfg.keys()
dict_keys(['rand_seed', 'dt'])

>>> cfg['dt']['ini']['root']
'/tmp/data'

>>> {**cfg['dt']['ini'], 'download': True}
{'root': '/tmp/data', 'train': True, 'download': True}
```

å¯ä»¥ç”¨`dict`ï¼ˆyamlã€jsonï¼‰æˆ–`dict`çš„å­ç±»ï¼ˆYACSã€mmcv.Configï¼‰æ¥åˆå§‹åŒ–`Config`å¯¹è±¡ï¼š
```text
>>> Config({'rand_seed': 0, 'dt': {'cls': MNIST, 'ini': {'root': '/tmp/data', 'train': True}}})
{'rand_seed': 0, 'dt': {'cls': <class 'torchvision.datasets.mnist.MNIST'>, 'ini': {'root': '/tmp/data', 'train': True}}}
```

ç”¨`.`æ“ä½œè¯»å†™`cfg`æ›´åŠ æ¸…æ™°ã€‚ä¾‹å¦‚ï¼Œä¸‹é¢çš„ä»£ç æ ¹æ®é…ç½®ï¼Œåˆ›å»ºå¹¶åˆå§‹åŒ–`MNIST`æ•°æ®é›†ï¼š
```text
>>> dataset = cfg.dt.cls(**cfg.dt.ini)
>>> dataset
Dataset MNIST
    Number of datapoints: 60000
    Root location: /tmp/data
    Split: Train
```
è‹¥è®¿é—®ä¸å­˜åœ¨çš„é”®ï¼Œä¼šè¿”å›ä¸€ä¸ªä¸€æ¬¡æ€§ç©ºå­—å…¸ï¼Œåº”å½“å°†å…¶è§†ä½œ`False`ï¼š
```text
>>> cfg.not_exit
{}
```
åœ¨[ä¸»ä»£ç ](alchemy_cat/dl_config/examples/train.py)ä¸­ï¼Œä½¿ç”¨ä¸‹é¢ä»£ç åŠ è½½é…ç½®ï¼š
```python
# # [INCOMPLETE] -- train.py --

from alchemy_cat.dl_config import load_config
cfg = load_config('configs/mnist/base/cfg.py', experiments_root='/tmp/experiment', config_root='configs')
# ... Code Omitted.
torch.save(model.state_dict(), f"{cfg.rslt_dir}/model_{epoch}.pth")  # Save all experiment results to cfg.rslt_dir.
```
`load_config`ä»`configs/mnist/base/cfg.py`ä¸­å¯¼å…¥`cfg`ï¼Œå¹¶å¤„ç†ç»§æ‰¿ã€ä¾èµ–ã€‚ç»™å®šå®éªŒæ ¹ç›®å½•`experiments_root`å’Œé…ç½®æ ¹ç›®å½•å’Œ`config_root`ï¼Œ`load_config`ä¼šè‡ªåŠ¨åˆ›å»ºå®éªŒç›®å½•`experiment/mnist/base`å¹¶èµ‹å€¼ç»™`cfg.rslt_dir`ï¼Œä¸€åˆ‡å®éªŒç»“æœéƒ½åº”å½“ä¿å­˜åˆ°`cfg.rslt_dir`ä¸­ã€‚

åŠ è½½å‡ºçš„`cfg`é»˜è®¤æ˜¯åªè¯»çš„ï¼ˆ`cfg.is_frozen == True`ï¼‰ã€‚è‹¥è¦ä¿®æ”¹ï¼Œç”¨`cfg.unfreeze()`è§£å†»`cfg`ã€‚

### æœ¬ç« å°ç»“
* é…ç½®æ–‡ä»¶æä¾›ä¸€ä¸ª`Config`å¯¹è±¡`cfg`ï¼Œå…¶æœ¬è´¨æ˜¯ä¸€ä¸ªæ ‘ç»“æ„çš„åµŒå¥—å­—å…¸ï¼Œæ”¯æŒ`.`æ“ä½œè¯»å†™ã€‚
* `cfg`è®¿é—®ä¸å­˜åœ¨çš„é”®æ—¶ï¼Œè¿”å›ä¸€ä¸ªä¸€æ¬¡æ€§ç©ºå­—å…¸ï¼Œå¯å°†å…¶è§†ä½œ`False`ã€‚
* ä½¿ç”¨`load_config`å‡½æ•°åŠ è½½é…ç½®æ–‡ä»¶ï¼Œå®éªŒç›®å½•ä¼šè‡ªåŠ¨åˆ›å»ºï¼Œå…¶è·¯å¾„ä¼šèµ‹å€¼ç»™`cfg.rslt_dir`ã€‚

## ç»§æ‰¿
æ–°é…ç½®å¯ä»¥ç»§æ‰¿å·²æœ‰çš„åŸºé…ç½®ï¼Œå†™ä½œ`cfg = Config(caps='base_cfg.py')`ã€‚æ–°é…ç½®åªéœ€è¦†å†™æˆ–æ–°å¢é¡¹ç›®ï¼Œå…¶ä½™é¡¹ç›®å°†å¤ç”¨åŸºé…ç½®ã€‚å¦‚å¯¹[åŸºé…ç½®](alchemy_cat/dl_config/examples/configs/mnist/plain_usage/cfg.py)ï¼š
```python
# -- [INCOMPLETE] configs/mnist/plain_usage/cfg.py --

# ... Code Omitted.

cfg.loader.ini.batch_size = 128
cfg.loader.ini.num_workers = 2

cfg.opt.cls = optim.AdamW
cfg.opt.ini.lr = 0.01

# ... Code Omitted.
```
è‹¥è¦ç¿»å€`batch_size`ï¼Œ[æ–°é…ç½®](alchemy_cat/dl_config/examples/configs/mnist/plain_usage,2xbs/cfg.py)å¯å†™ä½œï¼š
```python
# -- configs/mnist/plain_usage,2xbs/cfg.py --

from alchemy_cat.dl_config import Config

cfg = Config(caps='configs/mnist/plain_usage/cfg.py')  # Inherit from base config.

cfg.loader.ini.batch_size = 128 * 2  # Double batch size.

cfg.opt.ini.lr = 0.01 * 2  # Linear scaling rule, see https://arxiv.org/abs/1706.02677
```
ç»§æ‰¿è¡Œä¸ºç±»ä¼¼äº`dict.update`ã€‚æ ¸å¿ƒåŒºåˆ«åœ¨äºï¼Œè‹¥æ–°é…ç½®å’ŒåŸºé…ç½®æœ‰åŒåé”®ï¼Œä¸”å€¼éƒ½æ˜¯`Config`å®ä¾‹ï¼ˆç§°ä½œâ€œå­é…ç½®æ ‘â€ï¼‰ï¼Œæˆ‘ä»¬é€’å½’åœ°åœ¨å­æ ‘é—´åš`update`ã€‚å› æ­¤ï¼Œæ–°é…ç½®èƒ½å¤Ÿåœ¨ç»§æ‰¿`cfg.loader.ini.num_workers`çš„åŒæ—¶ï¼Œä¿®æ”¹`cfg.loader.ini.batch_size`ã€‚
```text
>>> base_cfg = load_config('configs/mnist/plain_usage/cfg.py', create_rslt_dir=False)
>>> new_cfg = load_config('configs/mnist/plain_usage,2xbs/cfg.py', create_rslt_dir=False)
>>> base_cfg.loader.ini
{'batch_size': 128, 'num_workers': 2}
>>> new_cfg.loader.ini
{'batch_size': 256, 'num_workers': 2}
```
è‹¥æƒ³åœ¨æ–°é…ç½®ä¸­é‡å†™æ•´æ£µå­é…ç½®æ ‘ï¼Œå¯å°†è¯¥å­æ ‘ç½®ä¸º "override"ï¼Œ[ä¾‹å¦‚](alchemy_cat/dl_config/examples/configs/mnist/plain_usage,override_loader/cfg.py)ï¼š
```python
# -- configs/mnist/plain_usage,override_loader/cfg.py --

from alchemy_cat.dl_config import Config

cfg = Config(caps='configs/mnist/plain_usage/cfg.py')  # Inherit from base config.

cfg.loader.ini.override()  # Set subtree as whole.
cfg.loader.ini.shuffle = False
cfg.loader.ini.drop_last = False
```
æ­¤æ—¶ï¼Œ`cfg.loader.ini`å®Œå…¨ç”±æ–°é…ç½®å®šä¹‰ï¼š
```text
>>> base_cfg = load_config('configs/mnist/plain_usage/cfg.py', create_rslt_dir=False)
>>> new_cfg = load_config('configs/mnist/plain_usage,2xbs/cfg.py', create_rslt_dir=False)
>>> base_cfg.loader.ini
{'batch_size': 128, 'num_workers': 2}
>>> new_cfg.loader.ini
{'shuffle': False, 'drop_last': False}
```
è‡ªç„¶è€Œç„¶åœ°ï¼ŒåŸºé…ç½®å¯ä»¥ç»§æ‰¿è‡ªå¦ä¸€ä¸ªåŸºé…ç½®ï¼Œå³é“¾å¼ç»§æ‰¿ã€‚

å¤šç»§æ‰¿ä¹Ÿæ˜¯æ”¯æŒçš„ï¼Œå†™ä½œ`cfg = Config(caps=('base.py', 'patch1.py', 'patch2.py', ...))`ï¼Œå…¶å»ºç«‹ä¸€æ¡`base -> patch1 -> patch2 -> current cfg`çš„ç»§æ‰¿é“¾ã€‚é å³çš„åŸºé…ç½®å¸¸ä½œä¸ºè¡¥ä¸é¡¹ï¼Œæ‰¹é‡æ·»åŠ ä¸€ç»„é…ç½®ã€‚ä¾‹å¦‚ä¸‹é¢çš„[patch](alchemy_cat/dl_config/examples/configs/patches/cifar10.py)åŒ…æ‹¬äº† CIFAR10 æ•°æ®é›†æœ‰å…³é…ç½®:
```python
# -- configs/patches/cifar10.py --

import torchvision.transforms as T
from torchvision.datasets import CIFAR10

from alchemy_cat.dl_config import Config

cfg = Config()

cfg.dt.override()
cfg.dt.cls = CIFAR10
cfg.dt.ini.root = '/tmp/data'
cfg.dt.ini.transform = T.Compose([T.ToTensor(), T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
```
è¦æ”¹ç”¨ CIFAR10ï¼Œ[æ–°é…ç½®](alchemy_cat/dl_config/examples/configs/mnist/plain_usage,cifar10/cfg.py)åªéœ€è¦ç»§æ‰¿è¯¥è¡¥ä¸å³å¯ï¼š
```python
# -- configs/mnist/plain_usage,cifar10/cfg.py --

from alchemy_cat.dl_config import Config

cfg = Config(caps=('configs/mnist/plain_usage/cfg.py', 'alchemy_cat/dl_config/examples/configs/patches/cifar10.py'))
```
```text
>>> cfg = load_config('configs/mnist/plain_usage,cifar10/cfg.py', create_rslt_dir=False)
>>> cfg.dt
{'cls': torchvision.datasets.cifar.CIFAR10,
 'ini': {'root': '/tmp/data',
  'transform': Compose(
      ToTensor()
      Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
  )}}
```
> _ç»§æ‰¿çš„å®ç°ç»†èŠ‚_
> 
> ç»§æ‰¿æ—¶ï¼Œæˆ‘ä»¬å…ˆæ‹·è´æ•´æ£µåŸºé…ç½®æ ‘ï¼Œå†ä»¥æ–°é…ç½®æ›´æ–°ä¹‹ï¼Œç¡®ä¿æ–°é…ç½®å’ŒåŸºé…ç½®çš„æ ‘ç»“æ„ç›¸äº’éš”ç¦»â€”â€”å³å¢ã€åˆ ã€æ”¹æ–°é…ç½®ä¸ä¼šå½±å“åŸºé…ç½®ã€‚å› æ­¤ï¼Œæ›´å¤æ‚ç»§æ‰¿å…³ç³»ï¼Œå¦‚è±å½¢ç»§æ‰¿ä¹Ÿæ˜¯æ”¯æŒçš„ï¼Œåªæ˜¯ä¸å¤ªå¯è¯»ï¼Œä¸å»ºè®®ä½¿ç”¨ã€‚\
> æ³¨æ„ï¼Œå¶ç»“ç‚¹çš„å€¼è¢«å¼•ç”¨ä¼ é€’ï¼ŒåŸåœ°ä¿®æ”¹å°†å½±å“æ•´æ¡ç»§æ‰¿é“¾ã€‚

### æœ¬ç« å°ç»“
* æ–°é…ç½®å€ŸåŠ©ç»§æ‰¿æ¥å¤ç”¨åŸºé…ç½®ï¼Œå¹¶è¦†å†™ã€æ–°å¢ä¸€äº›é…ç½®é¡¹ã€‚
* æ–°é…ç½®é€’å½’åœ°æ›´æ–°åŸºé…ç½®ï¼Œä½¿ç”¨`Config.override`å¯ä»¥é€€å›åˆ°`dict.update`çš„æ›´æ–°æ–¹å¼ã€‚
* `Config`æ”¯æŒé“¾å¼ç»§æ‰¿å’Œå¤šç»§æ‰¿ï¼Œå¯å®ç°æ›´åŠ ç»†ç²’åº¦çš„å¤ç”¨ã€‚

## ä¾èµ–
[ä¸Šä¸€èŠ‚](#ç»§æ‰¿)çš„ä¾‹å­ä¸­ï¼Œä¿®æ”¹æ–°é…ç½®çš„æ‰¹æ¬¡å¤§å°æ—¶ï¼Œå­¦ä¹ ç‡ä¹Ÿéšä¹‹æ”¹å˜ã€‚è¿™ç§ä¸€ä¸ªé…ç½®é¡¹éšç€å¦ä¸€ä¸ªé…ç½®é¡¹çš„å˜åŒ–æƒ…å†µï¼Œç§°ä½œâ€œä¾èµ–â€ã€‚

ä¿®æ”¹æŸä¸ªé…ç½®é¡¹åï¼Œå¿˜è®°ä¿®æ”¹å®ƒçš„ä¾èµ–é¡¹ï¼Œæ˜¯éå¸¸å¸¸è§çš„ bugã€‚å¥½åœ¨ AlchemyCat å¯ä»¥å®šä¹‰ä¾èµ–é¡¹ï¼Œä¿®æ”¹ä¾èµ–çš„æºå¤´ï¼Œæ‰€æœ‰ä¾èµ–é¡¹å°†è‡ªåŠ¨æ›´æ–°ã€‚[ä¾‹å¦‚](alchemy_cat/dl_config/examples/configs/mnist/base/cfg.py)ï¼š
```python
# -- [INCOMPLETE] configs/mnist/base/cfg.py --

from alchemy_cat.dl_config import Config, DEP
# ... Code Omitted.

cfg.loader.ini.batch_size = 128
# ... Code Omitted.
cfg.opt.ini.lr = DEP(lambda c: c.loader.ini.batch_size // 128 * 0.01)  # Linear scaling rule.

# ... Code Omitted.
```
å­¦ä¹ ç‡`cfg.opt.ini.lr`ä½œä¸ºä¾èµ–é¡¹`DEP`ï¼Œå€ŸåŠ©æ‰¹æ¬¡å¤§å°`cfg.loader.ini.batch_size`ç®—å‡ºã€‚`DEP`æ¥å—ä¸€ä¸ªå‡½æ•°ï¼Œå‡½æ•°çš„å®å‚ä¸º`cfg`ï¼Œå¹¶è¿”å›ä¾èµ–é¡¹çš„å€¼ã€‚

åœ¨[æ–°é…ç½®](alchemy_cat/dl_config/examples/configs/mnist/base,2xbs/cfg.py)ä¸­ï¼Œæˆ‘ä»¬åªéœ€ä¿®æ”¹æ‰¹æ¬¡å¤§å°ï¼Œå­¦ä¹ ç‡ä¼šè‡ªåŠ¨æ›´æ–°ï¼š
```python
# -- configs/mnist/base,2xbs/cfg.py --

from alchemy_cat.dl_config import Config

cfg = Config(caps='configs/mnist/base/cfg.py')

cfg.loader.ini.batch_size = 128 * 2  # Double batch size, learning rate will be doubled automatically.
```
```text
>>> cfg = load_config('configs/mnist/base,2xbs/cfg.py', create_rslt_dir=False)
>>> cfg.loader.ini.batch_size
256
>>> cfg.opt.ini.lr
0.02
```
ä»¥ä¸‹æ˜¯ä¸€ä¸ªæ›´å¤æ‚çš„[ä¾‹å­](alchemy_cat/dl_config/examples/configs/mnist/base/cfg.py)ï¼š
```python
# -- configs/mnist/base/cfg.py --

# ... Code Omitted.

cfg.sched.epochs = 30
@cfg.sched.set_DEP(name='warm_epochs', priority=0)  # kwarg `name` is not necessary
def warm_epochs(c: Config) -> int:  # warm_epochs = 10% of total epochs
    return round(0.1 * c.sched.epochs)

cfg.sched.warm.cls = sched.LinearLR
cfg.sched.warm.ini.total_iters = DEP(lambda c: c.sched.warm_epochs, priority=1)
cfg.sched.warm.ini.start_factor = 1e-5
cfg.sched.warm.ini.end_factor = 1.

cfg.sched.main.cls = sched.CosineAnnealingLR
cfg.sched.main.ini.T_max = DEP(lambda c: c.sched.epochs - c.sched.warm.ini.total_iters,
                               priority=2)  # main_epochs = total_epochs - warm_epochs

# ... Code Omitted.
```
```text
>>> print(cfg.sched.to_txt(prefix='cfg.sched.'))  # A pretty print of the config tree.
cfg.sched = Config()
# ------- â†“ LEAVES â†“ ------- #
cfg.sched.epochs = 30
cfg.sched.warm_epochs = 3
cfg.sched.warm.cls = <class 'torch.optim.lr_scheduler.LinearLR'>
cfg.sched.warm.ini.total_iters = 3
cfg.sched.warm.ini.start_factor = 1e-05
cfg.sched.warm.ini.end_factor = 1.0
cfg.sched.main.cls = <class 'torch.optim.lr_scheduler.CosineAnnealingLR'>
cfg.sched.main.ini.T_max = 27
```
ä»£ç ä¸­ï¼Œæ€»è®­ç»ƒè½®æ¬¡`cfg.sched.epochs`æ˜¯ä¾èµ–æºå¤´ï¼Œé¢„çƒ­è½®æ¬¡ `cfg.sched.warm_epochs`æ˜¯æ€»è®­ç»ƒè½®æ¬¡çš„ 10%ï¼Œä¸»è½®æ¬¡`cfg.sched.main.ini.T_max`æ˜¯æ€»è®­ç»ƒè½®æ¬¡å‡å»é¢„çƒ­è½®æ¬¡ã€‚ä¿®æ”¹æ€»è®­ç»ƒè½®æ¬¡ï¼Œé¢„çƒ­è½®æ¬¡å’Œä¸»è½®æ¬¡ä¼šè‡ªåŠ¨æ›´æ–°ã€‚

ä¾èµ–é¡¹`cfg.sched.warm_epochs`ä½¿ç”¨`Config.set_DEP`è£…é¥°å™¨æ¥å®šä¹‰ã€‚è¢«è£…é¥°çš„å‡½æ•°å³`DEP`çš„é¦–ä¸ªå‚æ•°ï¼Œç”¨äºè®¡ç®—ä¾èµ–é¡¹ã€‚ä¾èµ–é¡¹çš„é”®åç”±å…³é”®å­—å‚æ•°`name`æŒ‡å®šï¼Œè‹¥ç¼ºçœï¼Œåˆ™ä¸ºè¢«é¥°å‡½æ•°ä¹‹åã€‚å½“ä¾èµ–é¡¹çš„è®¡ç®—è¾ƒä¸ºå¤æ‚æ—¶ï¼Œæ¨èä½¿ç”¨è£…é¥°å™¨æ¥å®šä¹‰ã€‚

å½“ä¾èµ–é¡¹ä¾èµ–å¦ä¸€ä¸ªä¾èµ–é¡¹æ—¶ï¼Œå¿…é¡»æŒ‰æ­£ç¡®é¡ºåºè®¡ç®—å®ƒä»¬ã€‚é»˜è®¤çš„è®¡ç®—é¡ºåºæ˜¯å®šä¹‰é¡ºåºã€‚`priority`å‚æ•°å¯ä»¥äººå·¥æŒ‡å®šè®¡ç®—é¡ºåºï¼š`priority`è¶Šå°ï¼Œè®¡ç®—è¶Šæ—©ã€‚è­¬å¦‚ä¸Šé¢`cfg.sched.warm_epochs`è¢«`cfg.sched.warm.ini.total_iters`ä¾èµ–ï¼Œåè€…åˆè¢«`cfg.sched.main.ini.T_max`ä¾èµ–ï¼Œæ•…ä»–ä»¬çš„`priority`ä¾æ¬¡å¢åŠ ã€‚

### æœ¬ç« å°ç»“
* å½“ä¸€ä¸ªé…ç½®é¡¹ä¾èµ–äºå¦ä¸€é¡¹æ—¶ï¼Œåº”å®šä¹‰å…¶ä¸ºä¾èµ–é¡¹ã€‚æ”¹å˜ä¾èµ–æºå¤´ï¼Œä¾èµ–é¡¹ä¼šæ ¹æ®è®¡ç®—å‡½æ•°è‡ªåŠ¨è®¡ç®—ã€‚
* ä¾èµ–é¡¹å¯ä»¥é€šè¿‡`DEP(...)`æˆ–`Config.set_DEP`è£…é¥°å™¨å®šä¹‰ã€‚
* ä¾èµ–é¡¹é—´ç›¸äº’ä¾èµ–æ—¶ï¼Œå¯é€šè¿‡`priority`å‚æ•°æŒ‡å®šè§£ç®—é¡ºåºï¼Œå¦åˆ™æŒ‰ç…§å®šä¹‰é¡ºåºè§£ç®—ã€‚

## ç»„åˆ
ç»„åˆæ˜¯å¦ä¸€ç§å¤ç”¨é…ç½®çš„æ–¹å¼ã€‚é¢„å®šä¹‰å¥½çš„å­é…ç½®æ ‘ï¼Œå¯ä»¥åƒç§¯æœ¨ä¸€æ ·ï¼Œç»„åˆå‡ºå®Œæ•´çš„é…ç½®ã€‚è­¬å¦‚ï¼Œä¸‹é¢çš„[å­é…ç½®æ ‘](alchemy_cat/dl_config/examples/configs/addons/linear_warm_cos_sched.py)å®šä¹‰äº†ä¸€å¥—å­¦ä¹ ç‡ç­–ç•¥ï¼š

```python
# -- configs/addons/linear_warm_cos_sched.py --
import torch.optim.lr_scheduler as sched

from alchemy_cat.dl_config import Config, DEP

cfg = Config()

cfg.epochs = 30

@cfg.set_DEP(priority=0)  # warm_epochs = 10% of total epochs
def warm_epochs(c: Config) -> int:
    return round(0.1 * c.epochs)

cfg.warm.cls = sched.LinearLR
cfg.warm.ini.total_iters = DEP(lambda c: c.warm_epochs, priority=1)
cfg.warm.ini.start_factor = 1e-5
cfg.warm.ini.end_factor = 1.

cfg.main.cls = sched.CosineAnnealingLR
cfg.main.ini.T_max = DEP(lambda c: c.epochs - c.warm.ini.total_iters,
                         priority=2)  # main_epochs = total_epochs - warm_epochs

```
åœ¨[æœ€ç»ˆé…ç½®](alchemy_cat/dl_config/examples/configs/mnist/base,sched_from_addon/cfg.py)ä¸­ï¼Œæˆ‘ä»¬ç›´æ¥ç»„åˆè¿™å¥—å­¦ä¹ ç‡ç­–ç•¥ï¼š
```python
# -- configs/mnist/base,sched_from_addon/cfg.py --
# ... Code Omitted.

cfg.sched = Config('configs/addons/linear_warm_cos_sched.py')

# ... Code Omitted.
```
```text
>>> print(cfg.sched.to_txt(prefix='cfg.sched.'))  # A pretty print of the config tree.
cfg.sched = Config()
# ------- â†“ LEAVES â†“ ------- #
cfg.sched.epochs = 30
cfg.sched.warm_epochs = 3
cfg.sched.warm.cls = <class 'torch.optim.lr_scheduler.LinearLR'>
cfg.sched.warm.ini.total_iters = 3
cfg.sched.warm.ini.start_factor = 1e-05
cfg.sched.warm.ini.end_factor = 1.0
cfg.sched.main.cls = <class 'torch.optim.lr_scheduler.CosineAnnealingLR'>
cfg.sched.main.ini.T_max = 27
```
çœ‹èµ·æ¥éå¸¸ç®€å•ï¼å°±æ˜¯å°†é¢„å®šä¹‰çš„å­é…ç½®æ ‘ï¼Œèµ‹å€¼/æŒ‚è½½åˆ°æœ€ç»ˆé…ç½®ã€‚`Config('path/to/cfg.py')`è¿”å›é…ç½®æ–‡ä»¶ä¸­ï¼Œ`cfg`å¯¹è±¡çš„æ‹·è´ï¼ˆä¸[ç»§æ‰¿](#ç»§æ‰¿)ä¸­ä¸€æ ·ï¼Œæ‹·è´æ ‘ç»“æ„ä»¥ä¿è¯å¯¹é…ç½®çš„ä¿®æ”¹ç›¸äº’éš”ç¦»ï¼‰ã€‚

> _ç»„åˆå’Œä¾èµ–çš„å®ç°ç»†èŠ‚_
>
> ç»†å¿ƒçš„è¯»è€…å¯èƒ½ä¼šç–‘æƒ‘ï¼Œ`DEP`å¦‚ä½•å†³å®šä¾èµ–é¡¹è®¡ç®—å‡½æ•°çš„å‚æ•°`c`ï¼Œå…·ä½“ä¼ å…¥å“ªä¸ª`Config`å¯¹è±¡ï¼Ÿåœ¨æœ¬ç« çš„ä¾‹å­ä¸­ï¼Œ`c`çš„å®å‚æ˜¯å­¦ä¹ ç‡å­é…ç½®ï¼Œå› æ­¤`cfg.warm.ini.total_iters`çš„è®¡ç®—å‡½æ•°ä¸º`lambda c: c.warm_epochs`ã€‚ç„¶è€Œï¼Œåœ¨[ä¸Šä¸€ç« ](#ä¾èµ–)çš„ä¾‹å­ä¸­ï¼Œ`c`æ˜¯å®Œæ•´é…ç½®ï¼Œ`cfg.sched.warm.ini.total_iters`çš„è®¡ç®—å‡½æ•°ä¸º`lambda c: c.sched.warm_epochs`ã€‚
> 
> å…¶å®ï¼Œ`c`çš„å®å‚æ˜¯é…ç½®æ ‘çš„æ ¹èŠ‚ç‚¹ï¼Œåœ¨è¯¥æ ‘ä¸Šï¼Œ`DEP`ç¬¬ä¸€æ¬¡è¢«æŒ‚è½½ã€‚`Config`åœ¨æ˜¯ä¸€æ£µåŒå‘æ ‘ï¼Œ`DEP`ç¬¬ä¸€æ¬¡è¢«æŒ‚è½½æ—¶ï¼Œä¼šä¸Šæº¯åˆ°æ ¹èŠ‚ç‚¹ï¼Œè®°å½•`DEP`åˆ°æ ¹çš„ç›¸å¯¹è·ç¦»ã€‚è®¡ç®—æ—¶ï¼Œä¸Šæº¯ç›¸åŒè·ç¦»ï¼Œæ‰¾åˆ°å¯¹åº”çš„é…ç½®æ ‘ï¼Œå¹¶ä¼ å…¥è®¡ç®—å‡½æ•°ã€‚
> 
> è¦é˜»æ­¢è¯¥é»˜è®¤è¡Œä¸ºï¼Œè®¾ç½®`DEP(lambda c: ..., rel=False)`ï¼Œæ­¤æ—¶`c`çš„å®å‚æ€»æ˜¯ä¸ºå®Œæ•´é…ç½®ã€‚

**æœ€ä½³å®è·µï¼šç»„åˆå’Œç»§æ‰¿ï¼Œå…¶ç›®çš„éƒ½æ˜¯å¤ç”¨é…ç½®ã€‚ç»„åˆæ›´åŠ çµæ´»ã€ä½è€¦åˆï¼Œåº”å½“ä¼˜å…ˆä½¿ç”¨ç»„åˆï¼Œå°½é‡å‡å°‘ç»§æ‰¿å±‚æ¬¡ã€‚**

### æœ¬ç« å°ç»“
* å¯ä»¥ç”¨é¢„å®šä¹‰çš„å­é…ç½®ï¼Œç»„åˆå‡ºæœ€ç»ˆé…ç½®ã€‚

## å®Œæ•´æ ·ä¾‹

<details>
<summary> å±•å¼€å®Œæ•´æ ·ä¾‹ </summary>

å­¦ä¹ ç‡ç›¸å…³çš„[å­é…ç½®æ ‘](alchemy_cat/dl_config/examples/configs/addons/linear_warm_cos_sched.py)ï¼š
```python
# -- configs/addons/linear_warm_cos_sched.py --

import torch.optim.lr_scheduler as sched

from alchemy_cat.dl_config import Config, DEP

cfg = Config()

cfg.epochs = 30

@cfg.set_DEP(priority=0)  # warm_epochs = 10% of total epochs
def warm_epochs(c: Config) -> int:
    return round(0.1 * c.epochs)

cfg.warm.cls = sched.LinearLR
cfg.warm.ini.total_iters = DEP(lambda c: c.warm_epochs, priority=1)
cfg.warm.ini.start_factor = 1e-5
cfg.warm.ini.end_factor = 1.

cfg.main.cls = sched.CosineAnnealingLR
cfg.main.ini.T_max = DEP(lambda c: c.epochs - c.warm.ini.total_iters,
                         priority=2)  # main_epochs = total_epochs - warm_epochs
```
ç»„åˆå¾—åˆ°çš„[åŸºé…ç½®](alchemy_cat/dl_config/examples/configs/mnist/base,sched_from_addon/cfg.py)ï¼š
```python
# -- configs/mnist/base/cfg.py --

import torchvision.models as model
import torchvision.transforms as T
from torch import optim
from torchvision.datasets import MNIST

from alchemy_cat.dl_config import Config, DEP

cfg = Config()

cfg.rand_seed = 0

# -* Set datasets.
cfg.dt.cls = MNIST
cfg.dt.ini.root = '/tmp/data'
cfg.dt.ini.transform = T.Compose([T.Grayscale(3), T.ToTensor(), T.Normalize((0.1307,), (0.3081,)),])

# -* Set data loader.
cfg.loader.ini.batch_size = 128
cfg.loader.ini.num_workers = 2

# -* Set model.
cfg.model.cls = model.resnet18
cfg.model.ini.num_classes = DEP(lambda c: len(c.dt.cls.classes))

# -* Set optimizer.
cfg.opt.cls = optim.AdamW
cfg.opt.ini.lr = DEP(lambda c: c.loader.ini.batch_size // 128 * 0.01)  # Linear scaling rule.

# -* Set scheduler.
cfg.sched = Config('configs/addons/linear_warm_cos_sched.py')

# -* Set logger.
cfg.log.save_interval = DEP(lambda c: c.sched.epochs // 5, priority=1)  # Save model at every 20% of total epochs.
```
ç»§æ‰¿è‡ªåŸºé…ç½®ï¼Œæ‰¹æ¬¡å¤§å°ç¿»å€ï¼Œè½®æ¬¡æ•°ç›®å‡åŠçš„[æ–°é…ç½®](alchemy_cat/dl_config/examples/configs/mnist/base,sched_from_addon,2xbs,2Ã·epo/cfg.py)ï¼š

```python
# -- configs/mnist/base,sched_from_addon,2xbs,2Ã·epo/cfg.py --

from alchemy_cat.dl_config import Config

cfg = Config(caps='configs/mnist/base,sched_from_addon/cfg.py')

cfg.loader.ini.batch_size = 256

cfg.sched.epochs = 15
```
æ³¨æ„ï¼Œä¾èµ–é¡¹å¦‚å­¦ä¹ ç‡ã€é¢„çƒ­è½®æ¬¡ã€ä¸»è½®æ¬¡éƒ½ä¼šè‡ªåŠ¨æ›´æ–°ï¼š
```text
>>> cfg = load_config('configs/mnist/base,sched_from_addon,2xbs,2Ã·epo/cfg.py', create_rslt_dir=False)
>>> print(cfg)
cfg = Config()
cfg.override(False).set_attribute('_cfgs_update_at_parser', ('configs/mnist/base,sched_from_addon/cfg.py',))
# ------- â†“ LEAVES â†“ ------- #
cfg.rand_seed = 0
cfg.dt.cls = <class 'torchvision.datasets.mnist.MNIST'>
cfg.dt.ini.root = '/tmp/data'
cfg.dt.ini.transform = Compose(
    Grayscale(num_output_channels=3)
    ToTensor()
    Normalize(mean=(0.1307,), std=(0.3081,))
)
cfg.loader.ini.batch_size = 256
cfg.loader.ini.num_workers = 2
cfg.model.cls = <function resnet18 at 0x7f5bcda68a40>
cfg.model.ini.num_classes = 10
cfg.opt.cls = <class 'torch.optim.adamw.AdamW'>
cfg.opt.ini.lr = 0.02
cfg.sched.epochs = 15
cfg.sched.warm_epochs = 2
cfg.sched.warm.cls = <class 'torch.optim.lr_scheduler.LinearLR'>
cfg.sched.warm.ini.total_iters = 2
cfg.sched.warm.ini.start_factor = 1e-05
cfg.sched.warm.ini.end_factor = 1.0
cfg.sched.main.cls = <class 'torch.optim.lr_scheduler.CosineAnnealingLR'>
cfg.sched.main.ini.T_max = 13
cfg.log.save_interval = 3
cfg.rslt_dir = 'mnist/base,sched_from_addon,2xbs,2Ã·epo'
```
[è®­ç»ƒä»£ç ](alchemy_cat/dl_config/examples/train.py)ï¼š
```python
# -- train.py --
import argparse
import json

import torch
import torch.nn.functional as F
from rich.progress import track
from torch.optim.lr_scheduler import SequentialLR

from alchemy_cat.dl_config import load_config
from utils import eval_model

parser = argparse.ArgumentParser(description='AlchemyCat MNIST Example')
parser.add_argument('-c', '--config', type=str, default='configs/mnist/base,sched_from_addon,2xbs,2Ã·epo/cfg.py')
args = parser.parse_args()

# Folder 'experiment/mnist/base' will be auto created by `load` and assigned to `cfg.rslt_dir`
cfg = load_config(args.config, experiments_root='/tmp/experiment', config_root='configs')
print(cfg)

torch.manual_seed(cfg.rand_seed)  # Use `cfg` to set random seed

dataset = cfg.dt.cls(**cfg.dt.ini)  # Use `cfg` to set dataset type and its initial parameters

# Use `cfg` to set changeable parameters of loader,
# other fixed parameter like `shuffle` is set in main code
loader = torch.utils.data.DataLoader(dataset, shuffle=True, **cfg.loader.ini)

model = cfg.model.cls(**cfg.model.ini).train().to('cuda')  # Use `cfg` to set model

# Use `cfg` to set optimizer, and get `model.parameters()` in run time
opt = cfg.opt.cls(model.parameters(), **cfg.opt.ini, weight_decay=0.)

# Use `cfg` to set warm and main scheduler, and `SequentialLR` to combine them
warm_sched = cfg.sched.warm.cls(opt, **cfg.sched.warm.ini)
main_sched = cfg.sched.main.cls(opt, **cfg.sched.main.ini)
sched = SequentialLR(opt, [warm_sched, main_sched], [cfg.sched.warm_epochs])

for epoch in range(1, cfg.sched.epochs + 1):  # train `cfg.sched.epochs` epochs
    for data, target in track(loader, description=f"Epoch {epoch}/{cfg.sched.epochs}"):
        F.cross_entropy(model(data.to('cuda')), target.to('cuda')).backward()
        opt.step()
        opt.zero_grad()

    sched.step()

    # If cfg.log is defined, save model to `cfg.rslt_dir` at every `cfg.log.save_interval`
    if cfg.log and epoch % cfg.log.save_interval == 0:
        torch.save(model.state_dict(), f"{cfg.rslt_dir}/model_{epoch}.pth")

    eval_model(model)

if cfg.log:
    eval_ret = eval_model(model)
    with open(f"{cfg.rslt_dir}/eval.json", 'w') as json_f:
        json.dump(eval_ret, json_f)
```

è¿è¡Œ`python train.py --config 'configs/mnist/base,sched_from_addon,2xbs,2Ã·epo/cfg.py'`ï¼Œå°†ä¼šæŒ‰ç…§é…ç½®æ–‡ä»¶ä¸­è®¾ç½®ï¼Œä½¿ç”¨`train.py`è®­ç»ƒï¼Œå¹¶å°†ç»“æœä¿å­˜åˆ°`/tmp/experiment/mnist/base,sched_from_addon,2xbs,2Ã·epo`ç›®å½•ä¸­ã€‚
</details>

## è‡ªåŠ¨è°ƒå‚
åœ¨[ä¸Šé¢çš„ä¾‹å­](#å®Œæ•´æ ·ä¾‹)ä¸­ï¼Œæ¯è¿è¡Œä¸€æ¬¡`python train.py --config path/to/cfg.py`ï¼Œå°±é’ˆå¯¹ä¸€ç»„å‚æ•°ï¼Œå¾—åˆ°ä¸€ä»½å¯¹åº”çš„å®éªŒç»“æœã€‚

ç„¶è€Œï¼Œå¾ˆå¤šæ—¶å€™ï¼Œæˆ‘ä»¬éœ€è¦ç½‘æ ¼æœç´¢å‚æ•°ç©ºé—´ï¼Œå¯»æ‰¾æœ€ä¼˜çš„å‚æ•°ç»„åˆã€‚è‹¥ä¸ºæ¯ä¸€ç»„å‚æ•°ç»„åˆéƒ½å†™ä¸€ä¸ªé…ç½®ï¼Œæ—¢è¾›è‹¦ä¹Ÿå®¹æ˜“å‡ºé”™ã€‚é‚£èƒ½ä¸èƒ½åœ¨ä¸€ä¸ªã€å¯è°ƒé…ç½®ã€ä¸­ï¼Œå®šä¹‰æ•´ä¸ªå‚æ•°ç©ºé—´ã€‚éšåè®©ç¨‹åºè‡ªåŠ¨åœ°éå†æ‰€æœ‰å‚æ•°ç»„åˆï¼Œå¯¹æ¯ç»„å‚æ•°ç”Ÿæˆä¸€ä¸ªé…ç½®ï¼Œè¿è¡Œé…ç½®ï¼Œå¹¶æ±‡æ€»æ‰€æœ‰å®éªŒç»“æœä»¥ä¾›æ¯”è¾ƒã€‚

è‡ªåŠ¨è°ƒå‚æœºéå†å¯è°ƒé…ç½®çš„å‚æ•°ç»„åˆï¼Œç”Ÿæˆ`N`ä¸ªå­é…ç½®ï¼Œè¿è¡Œå¾—åˆ°`N`ä¸ªå®éªŒè®°å½•ï¼Œå¹¶å°†æ‰€æœ‰å®éªŒç»“æœæ€»ç»“åˆ° excel è¡¨æ ¼ä¸­ï¼š
```text
config to be tuned T â”€â”€â”€> config C1 + algorithm code A â”€â”€â”€> reproducible experiment E1(C1, A) â”€â”€â”€> summary table S(T,A)
                     â”‚Â                                                                          â”‚Â  
                     â”œâ”€â”€> config C2 + algorithm code A â”€â”€â”€> reproducible experiment E1(C2, A) â”€â”€â”‚Â 
                    ...                                                                         ...
```
### å¯è°ƒé…ç½®
ä½¿ç”¨è‡ªåŠ¨è°ƒå‚æœºï¼Œé¦–å…ˆéœ€è¦å†™ä¸€ä¸ªå¯è°ƒé…ç½®ï¼š
```python
# -- configs/tune/tune_bs_epoch/cfg.py --

from alchemy_cat.dl_config import Cfg2Tune, Param2Tune

cfg = Cfg2Tune(caps='configs/mnist/base,sched_from_addon/cfg.py')

cfg.loader.ini.batch_size = Param2Tune([128, 256, 512])

cfg.sched.epochs = Param2Tune([5, 15])
```
å…¶å†™æ³•ä¸ä¸Šä¸€ç« ä¸­çš„[æ™®é€šé…ç½®](alchemy_cat/dl_config/examples/configs/mnist/base,sched_from_addon,2xbs,2Ã·epo/cfg.py)éå¸¸ç±»ä¼¼ï¼ŒåŒæ ·æ”¯æŒå±æ€§è¯»å†™ã€ç»§æ‰¿ã€ä¾èµ–ã€ç»„åˆç­‰ç‰¹æ€§ã€‚åŒºåˆ«åœ¨äºï¼š
* å¯è°ƒé…ç½®çš„çš„ç±»å‹æ˜¯`Config`çš„å­ç±»`Cfg2Tune`ã€‚
* å¯¹éœ€è¦ç½‘æ ¼æœç´¢çš„å‚æ•°ï¼Œå®šä¹‰ä¸º`Param2Tune([v1, v2, ...])`ï¼Œå…¶ä¸­`v1, v2, ...`æ˜¯å‚æ•°çš„å¯é€‰å€¼ã€‚

ä¸Šé¢çš„å¯è°ƒé…ç½®ï¼Œä¼šæœç´¢ä¸€ä¸ª 3Ã—2=6 å¤§å°çš„å‚æ•°ç©ºé—´ï¼Œå¹¶ç”Ÿæˆå¦‚ä¸‹6ä¸ªå­é…ç½®ï¼š
```text
batch_size  epochs  child_configs            
128         5       configs/tune/tune_bs_epoch/batch_size=128,epochs=5/cfg.pkl
            15      configs/tune/tune_bs_epoch/batch_size=128,epochs=15/cfg.pkl
256         5       configs/tune/tune_bs_epoch/batch_size=256,epochs=5/cfg.pkl
            15      configs/tune/tune_bs_epoch/batch_size=256,epochs=15/cfg.pkl
512         5       configs/tune/tune_bs_epoch/batch_size=512,epochs=5/cfg.pkl
            15      configs/tune/tune_bs_epoch/batch_size=512,epochs=15/cfg.pkl
```

è®¾ç½®`Param2Tune`çš„`priority`å‚æ•°ï¼Œå¯ä»¥æŒ‡å®šå‚æ•°çš„æœç´¢é¡ºåºã€‚é»˜è®¤ä¸ºå®šä¹‰é¡ºåºã€‚è®¾ç½®` optional_value_names`å‚æ•°ï¼Œå¯ä»¥ä¸ºå‚æ•°å€¼æŒ‡å®šå¯è¯»çš„åå­—ã€‚[ä¾‹å¦‚](alchemy_cat/dl_config/examples/configs/tune/tune_bs_epoch,pri,name/cfg.py)ï¼š

```python
# -- configs/tune/tune_bs_epoch,pri,name/cfg.py --

from alchemy_cat.dl_config import Cfg2Tune, Param2Tune

cfg = Cfg2Tune(caps='configs/mnist/base,sched_from_addon/cfg.py')

cfg.loader.ini.batch_size = Param2Tune([128, 256, 512], optional_value_names=['1xbs', '2xbs', '4xbs'], priority=1)

cfg.sched.epochs = Param2Tune([5, 15], priority=0)
```
å…¶æœç´¢ç©ºé—´ä¸ºï¼š
```text
epochs batch_size  child_configs                    
5      1xbs        configs/tune/tune_bs_epoch,pri,name/epochs=5,batch_size=1xbs/cfg.pkl
       2xbs        configs/tune/tune_bs_epoch,pri,name/epochs=5,batch_size=2xbs/cfg.pkl
       4xbs        configs/tune/tune_bs_epoch,pri,name/epochs=5,batch_size=4xbs/cfg.pkl
15     1xbs        configs/tune/tune_bs_epoch,pri,name/epochs=15,batch_size=1xbs/cfg.pkl
       2xbs        configs/tune/tune_bs_epoch,pri,name/epochs=15,batch_size=2xbs/cfg.pkl
       4xbs        configs/tune/tune_bs_epoch,pri,name/epochs=15,batch_size=4xbs/cfg.pk
```

æˆ‘ä»¬è¿˜å¯ä»¥åœ¨å‚æ•°é—´è®¾ç½®çº¦æŸæ¡ä»¶ï¼Œè£å‰ªæ‰ä¸éœ€è¦çš„å‚æ•°ç»„åˆï¼Œå¦‚ä¸‹é¢[ä¾‹å­](alchemy_cat/dl_config/examples/configs/tune/tune_bs_epoch,subject_to/cfg.py)çº¦æŸæ€»è¿­ä»£æ•°ä¸è¶…è¿‡ 15Ã—128ï¼š
```python
# -- configs/tune/tune_bs_epoch,subject_to/cfg.py --

from alchemy_cat.dl_config import Cfg2Tune, Param2Tune

cfg = Cfg2Tune(caps='configs/mnist/base,sched_from_addon/cfg.py')

cfg.loader.ini.batch_size = Param2Tune([128, 256, 512])

cfg.sched.epochs = Param2Tune([5, 15],
                              subject_to=lambda cur_val: cur_val * cfg.loader.ini.batch_size.cur_val <= 15 * 128)
```
å…¶æœç´¢ç©ºé—´ä¸ºï¼š
```text
batch_size epochs  child_configs                 
128        5       configs/tune/tune_bs_epoch,subject_to/batch_size=128,epochs=5/cfg.pkl  
           15      configs/tune/tune_bs_epoch,subject_to/batch_size=128,epochs=15/cfg.pkl
256        5       configs/tune/tune_bs_epoch,subject_to/batch_size=256,epochs=5/cfg.pkl
```

### è¿è¡Œè‡ªåŠ¨è°ƒå‚æœº
æˆ‘ä»¬è¿˜éœ€è¦å†™ä¸€å°æ®µè„šæœ¬æ¥è¿è¡Œè‡ªåŠ¨è°ƒå‚æœºï¼š
```python
# -- tune_train.py --
import argparse, json, os, subprocess, sys
from alchemy_cat.dl_config import Config, Cfg2TuneRunner

parser = argparse.ArgumentParser(description='Tuning AlchemyCat MNIST Example')
parser.add_argument('-c', '--cfg2tune', type=str)
args = parser.parse_args()

# Set `pool_size` to GPU num, will run `pool_size` of configs in parallel
runner = Cfg2TuneRunner(args.cfg2tune, experiment_root='/tmp/experiment', work_gpu_num=1)

@runner.register_work_fn  # How to run config
def work(pkl_idx: int, cfg: Config, cfg_pkl: str, cfg_rslt_dir: str, cuda_env: dict[str, str]) -> ...:
    subprocess.run([sys.executable, 'train.py', '-c', cfg_pkl], env=cuda_env)

@runner.register_gather_metric_fn  # How to gather metric for summary
def gather_metric(cfg: Config, cfg_rslt_dir: str, run_rslt: ..., param_comb: dict[str, tuple[..., str]]) -> dict[str, ...]:
    return json.load(open(os.path.join(cfg_rslt_dir, 'eval.json')))

runner.tuning()
```
ä¸Šé¢çš„è„šæœ¬æ‰§è¡Œäº†å¦‚ä¸‹æ“ä½œï¼š
* ä¼ å…¥å¯è°ƒé…ç½®çš„è·¯å¾„ï¼Œå®ä¾‹åŒ–è°ƒå‚æœº`runner = Cfg2TuneRunner(...)`ã€‚<br> 
è‡ªåŠ¨è°ƒå‚æœºé»˜è®¤é€ä¸ªè¿è¡Œå­é…ç½®ã€‚è®¾ç½®å‚æ•°`work_gpu_num`ï¼Œå¯ä»¥å¹¶è¡Œè¿è¡Œ`len(os.environ['CUDA_VISIBLE_DEVICES']) // work_gpu_num`ä¸ªå­é…ç½®ã€‚
* æ³¨å†Œå·¥ä½œå‡½æ•°ï¼Œè¯¥å‡½æ•°ç”¨äºè¿è¡Œå•ä¸ªå­é…ç½®ã€‚å‡½æ•°å‚æ•°ä¸ºï¼š
  * `pkl_idx`ï¼šå­é…ç½®çš„åºå·
  * `cfg`ï¼šå­é…ç½®
  * `cfg_pkl`ï¼šå­é…ç½®çš„ pickle ä¿å­˜è·¯å¾„
  * `cfg_rslt_dir`ï¼šå­é…ç½®çš„å®éªŒç›®å½•ã€‚
  * `cuda_env`ï¼šå¦‚æœè®¾ç½®äº†`work_gpu_num`ï¼Œé‚£ä¹ˆ`cuda_env`ä¼šä¸ºå¹¶è¡Œå­é…ç½®åˆ†é…ä¸é‡å çš„ `CUDA_VISIBLE_DEVICES` ç¯å¢ƒå˜é‡ã€‚

  ä¸€èˆ¬è€Œè¨€ï¼Œæˆ‘ä»¬åªéœ€è¦å°†`cfg_pkl`ä½œä¸ºé…ç½®æ–‡ä»¶ï¼ˆ`load_cfg`æ”¯æŒè¯»å– pickle ä¿å­˜çš„é…ç½®ï¼‰ä¼ å…¥è®­ç»ƒè„šæœ¬å³å¯ã€‚å¯¹æ·±åº¦å­¦ä¹ ä»»åŠ¡ï¼Œè¿˜éœ€è¦ä¸ºæ¯ä¸ªé…ç½®è®¾ç½®ä¸åŒçš„`CUDA_VISIBLE_DEVICES`ã€‚
* æ³¨å†Œæ±‡æ€»å‡½æ•°ã€‚æ±‡æ€»å‡½æ•°å¯¹æ¯ä¸ªå®éªŒç»“æœï¼Œè¿”å›æ ¼å¼ä¸º`{metric_name: metric_value}`çš„å­—å…¸ã€‚è°ƒå‚æœºä¼šè‡ªåŠ¨éå†æ‰€æœ‰å®éªŒç»“æœï¼Œæ±‡æ€»åˆ°ä¸€ä¸ªè¡¨æ ¼ä¸­ã€‚æ±‡æ€»å‡½æ•°æ¥å—å¦‚ä¸‹å‚æ•°ï¼š
  * `cfg`ï¼šå­é…ç½®
  * `cfg_rslt_dir`ï¼šå­é…ç½®çš„å®éªŒç›®å½•
  * `run_rslt`ï¼šå·¥ä½œå‡½æ•°çš„è¿”å›å€¼
  * `param_comb`ï¼šå­é…ç½®çš„å‚æ•°ç»„åˆã€‚

    ä¸€èˆ¬æˆ‘ä»¬åªéœ€è¦åˆ°`cfg_rslt_dir`ä¸­è¯»å–å®éªŒç»“æœå¹¶è¿”å›å³å¯ã€‚
* è°ƒç”¨`runner.tuning()`ï¼Œå¼€å§‹è‡ªåŠ¨è°ƒå‚ã€‚

è°ƒå‚ç»“æŸåï¼Œå°†æ‰“å°è°ƒå‚ç»“æœï¼š
```text
Metric Frame: 
                  test_loss    acc
batch_size epochs                 
128        5       1.993285  32.63
           15      0.016772  99.48
256        5       1.889874  37.11
           15      0.020811  99.49
512        5       1.790593  41.74
           15      0.024695  99.33

Saving Metric Frame at /tmp/experiment/tune/tune_bs_epoch/metric_frame.xlsx
```
æ­£å¦‚æç¤ºä¿¡æ¯æ‰€è¨€ï¼Œè°ƒå‚ç»“æœè¿˜ä¼šè¢«ä¿å­˜åˆ° `/tmp/experiment/tune/tune_bs_epoch/metric_frame.xlsx` è¡¨æ ¼ä¸­ï¼š
<p align = "center">
<img  src="https://github.com/HAL-42/AlchemyCat/raw/master/docs/figs/readme-cfg2tune-excel.png" width="400" />
</p>

**æœ€ä½³å®è·µï¼šè‡ªåŠ¨è°ƒå‚æœºç‹¬ç«‹äºæ ‡å‡†å·¥ä½œæµã€‚åœ¨å†™é…ç½®å’Œä»£ç æ—¶ï¼Œå…ˆä¸è¦è€ƒè™‘è°ƒå‚ã€‚è°ƒå‚æ—¶ï¼Œå†å†™ä¸€ç‚¹ç‚¹é¢å¤–çš„ä»£ç ï¼Œå®šä¹‰å‚æ•°ç©ºé—´ï¼ŒæŒ‡å®šç®—æ³•çš„è°ƒç”¨å’Œç»“æœçš„è·å–æ–¹å¼ã€‚è°ƒå‚å®Œæ¯•åï¼Œå‰¥ç¦»è°ƒå‚æœºï¼Œåªå‘å¸ƒæœ€ä¼˜çš„é…ç½®å’Œç®—æ³•ã€‚**

### å¦ä¸€ä¸ªä¾‹å­ï¼šåœ¨ MMCV ä¸­ä½¿ç”¨è‡ªåŠ¨è°ƒå‚ 
<details>
<summary> ä¸ MMCV ç»“åˆä½¿ç”¨ </summary>

AlchemyCat æ”¯æŒç›´æ¥è¯»å†™ MMCV é…ç½®ï¼Œå¯è°ƒé…ç½®å¯ä»¥å†™ä½œï¼š

```python
from alchemy_cat.dl_config import Cfg2Tune, Param2Tune

cfg = Cfg2Tune(caps='mmcv_configs/deeplabv3plus/deeplabv3plus_r50-d8_4xb2-40k_cityscapes-512x1024.py')

cfg.model.backbone.depth = Param2Tune([50, 101])
cfg.train_cfg.max_iters = Param2Tune([10_000, 20_000])
```

åœ¨å·¥ä½œå‡½æ•°ä¸­ï¼Œæˆ‘ä»¬è°ƒç”¨ MMCV å®˜æ–¹è®­ç»ƒè„šæœ¬`train.py`ã€‚ç”±äº`work`æ”¶åˆ°çš„`cfg`æ˜¯ AlchemyCat æ ¼å¼çš„ï¼Œæˆ‘ä»¬éœ€è¦å°†å…¶ä¿å­˜ä¸º MMCV æ ¼å¼çš„é…ç½®æ–‡ä»¶ï¼Œå†ä¼ é€’ç»™`train.py`ï¼š

```python
@runner.register_work_fn  # How to run config
def work(pkl_idx: int, cfg: Config, cfg_pkl: str, cfg_rslt_dir: str) -> ...:
    cfg.save_mmcv(mmcv_cfg_file := 'path/to/mmcv_format_cfg.py')
    subprocess.run([sys.executable, 'train.py', mmcv_cfg_file],
                   env=os.environ | {'CUDA_VISIBLE_DEVICE': f'pkl_idx % torch.cuda.device_count()'})
```

</details>

### æœ¬ç« å°ç»“
* å¯ä»¥åœ¨å¯è°ƒé…ç½®`Cfg2Tune`ä¸­ï¼Œä½¿ç”¨`Param2Tune`å®šä¹‰å‚æ•°ç©ºé—´ã€‚
* è‡ªåŠ¨è°ƒå‚æœº`Cfg2TuneRunner`ä¼šéå†å‚æ•°ç©ºé—´ï¼Œç”Ÿæˆå­é…ç½®ï¼Œè¿è¡Œå­é…ç½®ï¼Œå¹¶æ±‡æ€»å®éªŒç»“æœã€‚

## è¿›é˜¶

<details>
<summary> å±•å¼€è¿›é˜¶ </summary>

### ç¾åŒ–æ‰“å°
`Config`çš„`__str__`æ–¹æ³•è¢«é‡è½½ï¼Œä»¥`.`åˆ†éš”çš„é”®åï¼Œç¾è§‚åœ°æ‰“å°æ ‘ç»“æ„ï¼š

```text
>>> cfg = Config()
>>> cfg.foo.bar.a = 1
>>> cfg.bar.foo.b = ['str1', 'str2']
>>> cfg.whole.override()
>>> print(cfg)
cfg = Config()
cfg.whole.override(True)
# ------- â†“ LEAVES â†“ ------- #
cfg.foo.bar.a = 1
cfg.bar.foo.b = ['str1', 'str2']
```

å¦‚æœæ‰€æœ‰å¶èŠ‚ç‚¹éƒ½æ˜¯å†…ç½®ç±»å‹ï¼Œ`Config`çš„ç¾è§‚æ‰“å°è¾“å‡ºå¯ç›´æ¥ä½œä¸º python ä»£ç æ‰§è¡Œï¼Œå¹¶å¾—åˆ°ç›¸åŒçš„é…ç½®ï¼š
```text
>>> exec(cfg.to_txt(prefix='new_cfg.'), globals(), (l_dict := {}))
>>> l_dict['new_cfg'] == cfg
True
```

å¯¹ä¸åˆæ³•çš„å±æ€§åï¼Œ`Config`ä¼šå›é€€åˆ°`dict`çš„æ‰“å°æ ¼å¼ï¼š
```text
>>> cfg = Config()
>>> cfg['Invalid Attribute Name'].foo = 10
>>> cfg.bar['def'] = {'a': 1, 'b': 2}
>>> print(cfg)
cfg = Config()
# ------- â†“ LEAVES â†“ ------- #
cfg['Invalid Attribute Name'].foo = 10
cfg.bar['def'] = {'a': 1, 'b': 2}
```

### è‡ªåŠ¨æ•è·å®éªŒæ—¥å¿—
å¯¹æ·±åº¦å­¦ä¹ ä»»åŠ¡ï¼Œæˆ‘ä»¬å»ºè®®ç”¨`init_env`ä»£æ›¿`load_config`ï¼Œåœ¨åŠ è½½é…ç½®ä¹‹ä½™ï¼Œ`init_env`è¿˜å¯ä»¥åˆå§‹åŒ–æ·±åº¦å­¦ä¹ ç¯å¢ƒï¼Œè­¬å¦‚è®¾ç½® torch è®¾å¤‡ã€æ¢¯åº¦ã€éšæœºç§å­ã€åˆ†å¸ƒå¼è®­ç»ƒï¼š

```python
from alchemy_cat.torch_tools import init_env

if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument('-c', '--config', type=str)
    parser.add_argument('--local_rank', type=int, default=-1)
    args = parser.parse_args()
    
    device, cfg = init_env(config_path=args.config,             # config file pathï¼Œread to `cfg`
                           is_cuda=True,                        # if Trueï¼Œ`device` is cudaï¼Œelse cpu
                           is_benchmark=bool(args.benchmark),   # torch.backends.cudnn.benchmark = is_benchmark
                           is_train=True,                       # torch.set_grad_enabled(is_train)
                           experiments_root="experiment",       # root of experiment dir
                           rand_seed=True,                      # set python, numpy, torch rand seed. If True, read cfg.rand_seed as seed, else use actual parameter as rand seed. 
                           cv2_num_threads=0,                   # set cv2 num threads
                           verbosity=True,                      # print more env init info
                           log_stdout=True,                     # where fork stdout to log file
                           loguru_ini=True,                     # config a pretty loguru format
                           reproducibility=False,               # set pytorch to reproducible mode
                           local_rank=...,                      # dist.init_process_group(..., local_rank=local_rank)
                           silence_non_master_rank=True,        # if True, non-master rank will not print to stdout, but only log to file
                           is_debug=bool(args.is_debug))        # is debug mode
```
å¦‚æœ`log_stdout=True`ï¼Œ`init_env`è¿˜ä¼šå°†`sys.stdout`ã€`sys.stderr` fork ä¸€ä»½åˆ°æ—¥å¿—æ–‡ä»¶`cfg.rslt_dir/{local-time}.log`ä¸­ã€‚è¿™ä¸ä¼šå¹²æ‰°æ­£å¸¸çš„`print`ï¼Œä½†æ‰€æœ‰å±å¹•è¾“å‡ºéƒ½ä¼šåŒæ—¶è¢«è®°å½•åˆ°æ—¥å¿—ã€‚å› æ­¤ï¼Œä¸å†éœ€è¦æ‰‹åŠ¨å†™å…¥æ—¥å¿—ï¼Œå±å¹•æ‰€è§å³æ—¥å¿—æ‰€å¾—ã€‚

æ›´è¯¦ç»†ç”¨æ³•å¯å‚è§`init_env`çš„ docstringã€‚

### å±æ€§å­—å…¸
å¦‚æœæ‚¨æ˜¯ [addict](https://github.com/mewwts/addict) çš„ç”¨æˆ·ï¼Œæˆ‘ä»¬çš„`ADict`å¯ä»¥ä½œä¸º`addict.Dict`çš„ drop-in replacementï¼š`from alchemy_cat.dl_config import ADict as Dict`ã€‚

`ADict` å…·å¤‡ `addict.Dict` çš„æ‰€æœ‰æ¥å£ï¼Œä½†é‡æ–°å®ç°äº†æ‰€æœ‰æ–¹æ³•ï¼Œä¼˜åŒ–äº†æ‰§è¡Œæ•ˆç‡ï¼Œè¦†ç›–äº†æ›´å¤š corner caseï¼ˆå¦‚å¾ªç¯å¼•ç”¨ï¼‰ã€‚`Config`å…¶å®å°±æ˜¯`ADict`çš„å­ç±»ã€‚

å¦‚æœæ‚¨æ²¡æœ‰ä½¿ç”¨è¿‡`addict`ï¼Œå¯ä»¥è€ƒè™‘é˜…è¯»è¿™ä»½[æ–‡æ¡£](https://github.com/mewwts/addict)ã€‚ç ”ç©¶å‹ä»£ç å¸¸å¸¸ä¼šä¼ é€’å¤æ‚çš„å­—å…¸ç»“æ„ï¼Œ`addict.Dict`æˆ–`ADict`æ”¯æŒå±æ€§è¯»å†™å­—å…¸ï¼Œéå¸¸é€‚åˆå¤„ç†åµŒå¥—å­—å…¸ã€‚

### å¾ªç¯å¼•ç”¨
`ADict`å’Œ`Config`çš„åˆå§‹åŒ–ã€ç»§æ‰¿ã€ç»„åˆéœ€è¦ç”¨åˆ°ä¸€ç§åä¸º`branch_copy`çš„æ“ä½œï¼Œå…¶ä»‹äºæµ…æ‹·è´å’Œæ·±æ‹·è´ä¹‹é—´ï¼Œå³æ‹·è´æ ‘ç»“æ„ï¼Œä½†ä¸æ‹·è´å¶èŠ‚ç‚¹ã€‚`ADict.copy`ï¼Œ`Config.copy`ï¼Œ`copy.copy(cfg)`å‡ä¼šè°ƒç”¨`branch_copy`ï¼Œè€Œé`dict`çš„`copy`æ–¹æ³•ã€‚

ç†è®ºä¸Š`ADict.branch_copy`èƒ½å¤Ÿå¤„ç†å¾ªç¯å¼•ç”¨æƒ…å†µï¼Œè­¬å¦‚ï¼š
```text
>>> dic = {'num': 0,
           'lst': [1, 'str'],
           'sub_dic': {'sub_num': 3}}
>>> dic['lst'].append(dic['sub_dic'])
>>> dic['sub_dic']['parent'] = dic
>>> dic
{'num': 0,
 'lst': [1, 'str', {'sub_num': 3, 'parent': {...}}],
 'sub_dic': {'sub_num': 3, 'parent': {...}}}

>>> adic = ADict(dic)
>>> adic.sub_dic.parent is adic is not dic
True
>>> adic.lst[-1] is adic.sub_dic is not dic['sub_dic']
True
```
å¯¹`ADict`ä¸åŒï¼Œ`Config`çš„æ•°æ®ç»“æ„æ˜¯åŒå‘æ ‘ï¼Œè€Œå¾ªç¯å¼•ç”¨å°†æˆç¯ã€‚ä¸ºé¿å…æˆç¯ï¼Œè‹¥å­é…ç½®æ ‘è¢«å¤šæ¬¡æŒ‚è½½åˆ°ä¸åŒçˆ¶é…ç½®ï¼Œå­é…ç½®æ ‘ä¼šå…ˆæ‹·è´å¾—åˆ°ä¸€æ£µç‹¬ç«‹çš„é…ç½®æ ‘ï¼Œå†è¿›è¡ŒæŒ‚è½½ã€‚æ­£å¸¸ä½¿ç”¨ä¸‹ï¼Œé…ç½®æ ‘ä¸­ä¸ä¼šå‡ºç°å¾ªç¯å¼•ç”¨ã€‚

æ€»è€Œè¨€ä¹‹ï¼Œå°½ç®¡å¾ªç¯å¼•ç”¨æ˜¯è¢«æ”¯æŒçš„ï¼Œä¸è¿‡å³æ²¡æœ‰å¿…è¦ï¼Œä¹Ÿä¸æ¨èä½¿ç”¨ã€‚

### éå†é…ç½®æ ‘
`Config.named_branches`å’Œ`Config.named_ckl`åˆ†åˆ«éå†é…ç½®æ ‘çš„æ‰€æœ‰åˆ†æ”¯å’Œå¶èŠ‚ç‚¹ï¼ˆæ‰€åœ¨çš„åˆ†æ”¯ã€é”®åå’Œå€¼ï¼‰ï¼š
```text
>>> list(cfg.named_branches) 
[('', {'foo': {'bar': {'a': 1}},  
       'bar': {'foo': {'b': ['str1', 'str2']}},  
       'whole': {}}),
 ('foo', {'bar': {'a': 1}}),
 ('foo.bar', {'a': 1}),
 ('bar', {'foo': {'b': ['str1', 'str2']}}),
 ('bar.foo', {'b': ['str1', 'str2']}),
 ('whole', {})]
 
>>> list(cfg.ckl)
[({'a': 1}, 'a', 1), ({'b': ['str1', 'str2']}, 'b', ['str1', 'str2'])]
```

### æƒ°æ€§ç»§æ‰¿
```text
>>> from alchemy_cat.dl_config import Config
>>> cfg = Config(caps='configs/mnist/base,sched_from_addon/cfg.py')
>>> cfg.loader.ini.batch_size = 256
>>> cfg.sched.epochs = 15
>>> print(cfg)

cfg = Config()
cfg.override(False).set_attribute('_cfgs_update_at_parser', ('configs/mnist/base,sched_from_addon/cfg.py',))
# ------- â†“ LEAVES â†“ ------- #
cfg.loader.ini.batch_size = 256
cfg.sched.epochs = 15
```
ç»§æ‰¿æ—¶ï¼Œçˆ¶é…ç½®`caps`ä¸ä¼šè¢«ç«‹å³æ›´æ–°è¿‡æ¥ï¼Œè€Œæ˜¯ç­‰åˆ°`load_config`æ—¶æ‰ä¼šè¢«åŠ è½½ã€‚æƒ°æ€§ç»§æ‰¿ä½¿å¾—é…ç½®ç³»ç»Ÿå¯ä»¥é¸Ÿç°æ•´æ¡ç»§æ‰¿é“¾ï¼Œå°‘æ•°åŠŸèƒ½æœ‰èµ–äºæ­¤ã€‚

### ååŒGit
ç”±äº`config C + algorithm code A â€”â€”> reproducible experiment E(C, A)`ï¼Œæ„å‘³ç€å½“é…ç½®`C`å’Œç®—æ³•ä»£ç `A`ç¡®å®šæ—¶ï¼Œæ€»æ˜¯èƒ½å¤ç°å®éªŒ`E`ã€‚å› æ­¤ï¼Œå»ºè®®å°†é…ç½®æ–‡ä»¶å’Œç®—æ³•ä»£ç ä¸€åŒæäº¤åˆ° Git ä»“åº“ä¸­ï¼Œä»¥ä¾¿æ—¥åå¤ç°å®éªŒã€‚

æˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ª[è„šæœ¬](alchemy_cat/torch_tools/scripts/tag_exps.py)ï¼Œè¿è¡Œ`pyhon -m alchemy_cat.torch_tools.scripts.tag_exps -s commit_ID -a commit_ID`ï¼Œå°†äº¤äº’å¼åœ°åˆ—å‡ºè¯¥ commit æ–°å¢çš„é…ç½®ï¼Œå¹¶æŒ‰ç…§é…ç½®è·¯å¾„ç»™ commit æ‰“ä¸Šæ ‡ç­¾ã€‚è¿™æœ‰åŠ©äºå¿«é€Ÿå›æº¯å†å²ä¸ŠæŸä¸ªå®éªŒçš„é…ç½®å’Œç®—æ³•ã€‚

### ä¸ºå­ä»»åŠ¡æ‰‹åŠ¨åˆ†é…æ˜¾å¡
`Cfg2TuneRunner`çš„`work`å‡½æ•°æœ‰æ—¶éœ€è¦ä¸ºå­è¿›ç¨‹åˆ†é…æ˜¾å¡ã€‚é™¤äº†ä½¿ç”¨`cuda_env`å‚æ•°ï¼Œè¿˜å¯ä»¥ä½¿ç”¨`allocate_cuda_by_group_rank`ï¼Œæ ¹æ®`pkl_idx`æ‰‹åŠ¨åˆ†é…ç©ºé—²æ˜¾å¡ï¼Œï¼š
```python
from alchemy_cat.cuda_tools import allocate_cuda_by_group_rank

# ... Code before

@runner.register_work_fn  # How to run config
def work(pkl_idx: int, cfg: Config, cfg_pkl: str, cfg_rslt_dir: str, cuda_env: dict[str, str]) -> ...:
    current_cudas, env_with_current_cuda = allocate_cuda_by_group_rank(group_rank=pkl_idx, group_cuda_num=2, block=True, verbosity=True)
    subprocess.run([sys.executable, 'train.py', '-c', cfg_pkl], env=env_with_current_cuda)

# ... Code after
```
`group_rank`ä¸€èˆ¬ä¸º`pkl_idx`ï¼Œ`group_cuda_num`ä¸ºä»»åŠ¡æ‰€éœ€æ˜¾å¡æ•°é‡ã€‚`block`ä¸º`True`æ—¶ï¼Œè‹¥åˆ†é…çš„æ˜¾å¡è¢«å ç”¨ï¼Œä¼šé˜»å¡ç›´åˆ°æœ‰ç©ºé—²ã€‚`verbosity`ä¸º`True`æ—¶ï¼Œä¼šæ‰“å°é˜»å¡æƒ…å†µã€‚

è¿”å›å€¼`current_cudas`æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«äº†åˆ†é…çš„æ˜¾å¡å·ã€‚`env_with_current_cuda`æ˜¯è®¾ç½®äº†`CUDA_VISIBLE_DEVICES`çš„ç¯å¢ƒå˜é‡å­—å…¸ï¼Œå¯ç›´æ¥ä¼ å…¥`subprocess.run`çš„`env`å‚æ•°ã€‚

### åŒ¿åå‡½æ•°æ— æ³• pickle é—®é¢˜
`Cfg2Tune`ç”Ÿæˆçš„å­é…ç½®ä¼šè¢« pickle ä¿å­˜ã€‚ç„¶è€Œï¼Œè‹¥`Cfg2Tune`å®šä¹‰äº†å½¢ä¼¼`DEP(lambda c: ...)`çš„ä¾èµ–é¡¹ï¼Œæ‰€å­˜å‚¨çš„åŒ¿åå‡½æ•°æ— æ³•è¢« pickleã€‚å˜é€šæ–¹æ³•æœ‰ï¼š
* é…åˆè£…é¥°å™¨`@Config.set_DEP`ï¼Œå°†ä¾èµ–é¡¹çš„è®¡ç®—å‡½æ•°å®šä¹‰ä¸ºä¸€ä¸ªå…¨å±€å‡½æ•°ã€‚
* å°†ä¾èµ–é¡¹çš„è®¡ç®—å‡½æ•°å®šä¹‰åœ¨ä¸€ä¸ªç‹¬ç«‹çš„æ¨¡å—ä¸­ï¼Œç„¶åå†ä¼ é€’ç»™`DEP`ã€‚
* åœ¨çˆ¶é…ç½®`caps`ä¸­å®šä¹‰ä¾èµ–é¡¹ã€‚ç”±äºç»§æ‰¿çš„å¤„ç†æ˜¯æƒ°æ€§çš„ï¼Œ`Cfg2Tune`ç”Ÿæˆçš„å­é…ç½®æš‚æ—¶ä¸åŒ…å«ä¾èµ–é¡¹ã€‚
* å¦‚æœä¾èµ–æºæ˜¯å¯è°ƒå‚æ•°ï¼Œå¯ä½¿ç”¨ç‰¹æ®Šçš„ä¾èµ–é¡¹`P_DEP`ï¼Œå®ƒå°†äº`Cfg2Tune`ç”Ÿæˆå­é…ç½®åã€ä¿å­˜ä¸º pickle å‰è§£ç®—ã€‚

### å…³äºç»§æ‰¿çš„æ›´å¤šæŠ€å·§

#### ç»§æ‰¿æ—¶åˆ é™¤
`Config.empty_leaf()`ç»“åˆäº†`Config.clear()`å’Œ`Config.override()`ï¼Œå¯ä»¥å¾—åˆ°ä¸€æ£µç©ºä¸” "override" çš„å­æ ‘ã€‚è¿™å¸¸ç”¨äºåœ¨ç»§æ‰¿æ—¶è¡¨ç¤ºã€åˆ é™¤ã€è¯­ä¹‰ï¼Œå³ç”¨ä¸€ä¸ªç©ºé…ç½®ï¼Œè¦†ç›–æ‰åŸºé…ç½®çš„æŸé¢—å­é…ç½®æ ‘ã€‚

#### `update`æ–¹æ³•
`cfg`æ˜¯ä¸€ä¸ª`Config`å®ä¾‹ï¼Œ`base_cfg`æ˜¯ä¸€ä¸ª`dict`å®ä¾‹ï¼Œ`cfg.dict_update(base_cfg)`ã€`cfg.update(base_cfg)`ã€`cfg |= base_cfg`çš„æ•ˆæœä¸è®©`Config(base_cfg)`ç»§æ‰¿`cfg`ç±»ä¼¼ã€‚

`cfg.dict_update(base_cfg, incremental=True)`åˆ™ç¡®ä¿åªåšå¢é‡å¼æ›´æ–°â€”â€”å³åªä¼šå¢åŠ `cfg`ä¸­ä¸å­˜åœ¨çš„é”®ï¼Œè€Œä¸ä¼šè¦†ç›–å·²æœ‰é”®ã€‚

</details>
